{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes clean and common data and compute all sort of KPIs based on offer caracteriscs (essentially NLP on descriptions). Then store the processed data in a common file which contains all the data since the beginning of the project.\n",
    "\n",
    "- READ : *.csv - data/new_tmp_data : new_clean_data.csv\n",
    "- READ : *.csv - data/processed_data : already processed data\n",
    "- WRITE : *.csv - data/processed_data/history : new processed_data (historic)\n",
    "- WRITE : *.csv - data/processed_data : concatenation of previous processed data and new processed data\n",
    "- WRITE : *.json - data/processed_data : wright the IDs stored by websites so that the first script know which data to keep from the fresh scrapped data\n",
    "\n",
    "In case of new processing to apply to all data follow this procedure, run the new_process.py scriptun `utils`folder, it will :\n",
    "- Delete processed_data and json IDs  \n",
    "- Make a concatenation of all historical raw_data and\n",
    "- set them in the new_tmp_data\n",
    "- Run this notebook to compute all processing on all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prix</th>\n",
       "      <th>surface</th>\n",
       "      <th>prix_m2</th>\n",
       "      <th>ville</th>\n",
       "      <th>code_postal</th>\n",
       "      <th>origine</th>\n",
       "      <th>dept</th>\n",
       "      <th>id_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prix, surface, prix_m2, ville, code_postal, origine, dept, id_]\n",
       "Index: []"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDER = 'data/new_tmp_data'\n",
    "TITLE = 'new_clean_data.csv'\n",
    "path = '{}/{}'.format(FOLDER, TITLE)\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Dealing with Paris\n",
    "    df['ville'] = df[['ville', 'code_postal']].apply(paris_decompo, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Dealing with viager\n",
    "    df['viager'] = df['contenu'].apply(lambda x: True if 'viager' in x else False)\n",
    "    df_viager = df[df['viager']]\n",
    "    df_viager_new = df_viager[df_viager.index>last_analyse]\n",
    "    print('Nombre d\\'appartement en viager au total', df_viager.shape[0])\n",
    "    print('Nombre d\\'appartement en viager nouveau', df_viager_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_old():\n",
    "    path = 'data/processed_data/process_data.csv'\n",
    "    if os.path.isfile(path):\n",
    "        return pd.read_csv(path)\n",
    "    print('No old processed data at : \\n{}'.format(path))\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_process_data_history(df):\n",
    "    folder = 'data/processed_data/history'\n",
    "    now = f.get_now()\n",
    "    path = '{}/processed_{}.csv'.format(folder, now)\n",
    "    df.to_csv(path, header=True, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_process_with_previous(df_new, df_old):\n",
    "    folder = 'data/processed_data'\n",
    "    path = '{}/process_data.csv'.format(folder)\n",
    "    df_concat = (pd.concat([df_old, df_new], sort=True)\n",
    "                 .drop_duplicates()\n",
    "                 .reset_index(drop=True))\n",
    "    df_concat.to_csv(path, index=False)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ids(df_concat):\n",
    "    for source in ['lbc', 'sl', 'pv']:\n",
    "        list_lbc = json.loads(df_concat[df_concat['origine']==source]['id_'].to_json())\n",
    "        with open('data/processed_data/list_{}_id.json'.format(source), 'w') as fp:\n",
    "            json.dump(list_lbc, fp)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading old data\n",
    "df_old = load_old()\n",
    "\n",
    "# Saving all results\n",
    "df_concat = (df_processed.pipe(save_process_data_history)\n",
    "             .pipe(concat_process_with_previous, df_old)\n",
    "             .pipe(save_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save_process_data_history(df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_concat = concat_process_with_previous(df_new, df_old)\n",
    "#save_ids(df_concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
